# Merchant Management System (MMS)

## Overview
The Merchant Management System (MMS) is a comprehensive web application for enterprise-scale merchant relationship management, transaction processing, and business data analysis. It provides robust solutions for handling large datasets, including merchant management, transaction processing, file uploads, data analytics, and automated backup, with a focus on rapid responses. The system aims to deliver a robust, scalable, and user-friendly platform for efficient business operations.

## User Preferences
Preferred communication style: Simple, everyday language.
Timezone: CST (America/Chicago) - All dates and times should display in Central Time.
Critical System Requirement: "Never re-fresh" policy - all auto-refresh functionality permanently disabled and verified working.

## System Architecture
MMS employs a modern client-server architecture prioritizing performance, scalability, and maintainability.

### UI/UX Decisions
The UI/UX emphasizes a modern, professional, and intuitive experience using TailwindCSS and shadcn/ui. This includes consistent design, responsive layouts, intuitive interactions (e.g., interactive heat maps, sorting, progress indicators, color-coded status), a PDF reporting system, dynamic MCC schema-driven forms for TSYS Risk & Configuration Fields, and consolidated automation controls. Enhanced TDDF1 monthly view includes clickable rows navigating to daily dashboards and dedicated duplicate file cleanup per date.

### Technical Implementations
- **Frontend**: React-based Single Page Application (SPA) with TypeScript.
- **Backend**: RESTful Express.js API.
- **Database**: PostgreSQL with Drizzle ORM, utilizing database-level locking for concurrency.
- **Data Management**: Unified "never expire" cache, dynamic aggregation with performance tiers, and hybrid storage (object storage for raw data, database for structured data). Includes self-repairing cache and TDDF1 hybrid pre-cache architecture for dashboard performance.
- **File Processing**: Robust, automated 5-stage pipeline for large files (CSV, TSV, JSON, TDDF) with metadata capture, failed file recovery, TDDF-specific switch-based processing, and duplicate file upload prevention with line-level deduplication. Soft-delete is implemented for TDDF file uploads with persistent audit logging. Step 6 processing includes retry limits (MAX_STEP6_RETRIES=3) and timeout protection (5-minute timeout) to prevent infinite retry loops and memory exhaustion.
- **TDDF Specifics**: Shared TDDF resource architecture, enhanced metadata system, comprehensive pre-cache for record tabs, improved BH → DT → G2 relationship display, and flag-based archiving with seamless restoration. TDDF JSONB query performance is optimized via expression indexes and quarterly table partitioning.
- **Quarterly Table Partitioning**: The `tddf_jsonb` master table uses PostgreSQL quarterly range partitioning on `tddf_processing_date` to optimize query performance for multi-year datasets. Each quarter is a separate partition (e.g., 2024-Q1, 2024-Q2), enabling partition pruning where queries scan only relevant quarters instead of the entire table. Auto-creation functions (`create_quarterly_partition`, `ensure_future_partitions`) automatically create future partitions. The table uses a composite primary key `(id, tddf_processing_date)` required for partitioning, with an additional `id`-only index for cross-partition queries. Combined with expression indexes on JSONB fields, this delivers 2000x query performance improvements (monthly dashboard queries: ~70ms vs 2-3 minutes).
- **Operational Features**: Cross-environment storage management, startup TDDF cache validation, production self-correcting database, editable MCC schema configuration, and enhanced auto-retry systems.
- **Error Recovery & Admin Tools**: Multi-phase error recovery system with atomic transaction-based `/api/uploader/reset-errors` endpoint for resetting files from error phase to encoded. Enhanced `/api/admin/reset-stuck-step6-files` endpoint supports multiple phases (validating, identified, processing, error) with phase-specific reset logic (validating/identified → uploaded, processing/error → encoded). Warning telemetry only incremented for processing/error resets, not normal flow retries. All reset operations use database transactions with FOR UPDATE locks and race condition detection. Enhanced status filtering UI includes all processing phases (validating, processing, error) with unlimited pagination support (50, 100, 200 items per page). **Validating Phase Auto-Recovery**: Comprehensive try/catch wrapping entire Step 6 validation pipeline (line processing, batch inserts, duplicate cleanup, merchant updates) automatically reverts failed files to 'encoded' status with retry_count increment. MMS Watcher pipeline recovery detects files stuck in validating phase (>5 min) with concurrent processing guards: skips active workers <10min, force-releases stale workers >10min, implements 3-retry limit before marking failed. Structured telemetry tracks detected/skipped/reset/failed counts with duration metrics for observability.
- **User Authentication**: Microsoft Azure AD OAuth with optional Duo two-factor authentication, alongside existing username/password authentication.
- **Data Precision**: Enhanced currency formatting to consistently display two decimal places.
- **Terminal Management**: Efficient UPSERT logic for terminal imports with unique constraints and automatic timestamp updates. TDDF Step 6 processing creates/updates terminals from DT records (positions 277-284) with full audit trail tracking including `update_source` (format: "TDDF: filename Line: 12345"), `created_by`/`updated_by` (format: "STEP6:upload_id"), and automatic timestamps. Terminal IDs starting with '7' or '0' are converted to V-number format (e.g., 75679867 → V5679867) for v_number storage while preserving original format in terminal_id field.
- **Analytics**: Comprehensive daily merchant breakdown dashboard with full record type details, optimized with multiple performance indexes. Includes a compact monthly calendar heat map for terminal activity. Standardized `mcc` property usage across backend and frontend for accurate reporting. Merchant list now displays pre-cached "Last Batch" (filename and date) and "Last Transaction" (amount and date) instead of aggregated daily/monthly statistics for improved performance.
- **Processing Page**: Real-time TDDF processing monitoring dashboard with modern JSONB-backed endpoints. Four API endpoints query dev_uploaded_files and dev_tddf_jsonb directly: real-time-stats (file counts + record breakdown from last hour), performance-kpis (records per minute from last 10 minutes with DT/BH/P1/P2 breakdown), queue-status (currently processing files), and performance-chart-history (historical minute-by-minute data with 1440-point limit). Critical created_at index on dev_tddf_jsonb ensures sub-second response times. RecordsPerMinuteChart displays stacked bar visualization of record types processed per minute.
- **Batch Uploaders**: PowerShell and Python-based batch file uploaders with API key authentication, automatic chunking for large files, queue status monitoring, and retry logic.
- **API Key Usage Tracking**: Comprehensive monitoring system tracking `last_used` timestamp, `last_used_ip`, and `request_count` for all API key authenticated requests.
- **Connection Logging & IP Blocking**: Global middleware logs all API requests to a `dev_connection_log` table. An IP blocking system (`dev_ip_blocklist` table) allows administrators to block malicious IPs.
- **Host Approval System**: Security-enhanced upload access control based on hostname + API key combinations, requiring administrator approval for new hosts.
- **Dynamic Verbose Logging API**: Runtime-controllable logging system with API endpoints to adjust verbosity for authentication, navigation, uploader, charts, TDDF processing, and database modules.
- **Database Health Monitoring**: Production-ready database health check system with API endpoints for latency tests, full health checks (connection, tables, indexes, orphaned records, stuck files, cache integrity), and schema validation.
- **Storage Management System**: Comprehensive object storage management and cleanup system with a master object keys database. Provides API endpoints for storage statistics, object listing, duplicate detection, orphaned object scanning, purging, and manual deletion. Includes a frontend for managing storage.
- **Pre-Cache Management System**: Fully operational background pre-calculation system for monthly TDDF data enabling instant dashboard loading through cache-first architecture. Successfully caches 38 months of data (2022-10 through 2025-11, 6.4M+ records across 1,640 files) with average build time of 4.5 seconds per month. Includes database tables (dev_tddf1_monthly_cache with comprehensive columns for metrics and metadata, dev_pre_cache_runs for execution tracking with records_cached field), PreCacheService for cache operations with proper DATE/TEXT parameter separation, automated background job using node-schedule (2-hour cron, configurable), and secure handler-map pattern in REST API routes preventing SQL injection. Pre-Cache Management UI provides manual cache refresh, execution history, and cache statistics. Cache invalidation hooks into Step 6 processing completion for auto-refresh of affected months. **Monthly Cache Management**: Dedicated UI tab provides comprehensive monthly cache inspection with table view displaying all 38 cached months, detailed metrics per month (file counts, record counts, transaction amounts, net deposits), status badges, and individual month detail views via modal dialog. Detail dialog includes four tabs: Summary (files & records, financial metrics), Totals JSON, Daily Breakdown JSON, and Comparison JSON for full data transparency. Per-month rebuild functionality available via REST API (POST /api/pre-cache/monthly-cache/:year/:month/rebuild). Backend endpoints: GET /api/pre-cache/monthly-cache (list all), GET /api/pre-cache/monthly-cache/:year/:month (detailed view). All queries use handler-map security pattern to prevent SQL injection. **Background Rebuild Tracking**: Real-time rebuild job tracking system with RebuildJobTracker service tracking active cache rebuilds in-memory with 5-minute retention for completed jobs. Race condition prevention via timeout clearing and jobId verification ensures multiple rapid rebuilds of the same month work correctly. API endpoint GET /api/pre-cache/rebuild-status returns JSON-serializable job status (excludes cleanupTimeout to prevent circular structure errors) with activeCount for real-time monitoring. Frontend implements adaptive polling (5s when active jobs, 30s otherwise) with useMemo-based status merging to display dynamic badges (active, rebuilding with spinner, error). Rebuild buttons automatically disable during active rebuilds to prevent duplicate jobs. Users can navigate away during background rebuilds and return to see updated status. System is production-ready for single-instance deployments with job visibility resetting on process restart (long-term auditing via dev_pre_cache_runs table). **Dual Upload Tracking Architecture (Nov 2025)**: System uses two upload tracking tables: dev_uploaded_files (legacy, INTEGER ids for pre-Nov 2025 data) and dev_uploader_uploads (modern, TEXT ids like "uploader_1763060125183_s74jwja9w" for Nov 2025+). Cache rebuild queries use CASE-based LEFT JOIN predicates to handle mixed upload_id types without type casting errors: `LEFT JOIN dev_uploaded_files u1 ON CASE WHEN t.upload_id ~ '^[0-9]+$' THEN t.upload_id::integer = u1.id ELSE FALSE END` prevents PostgreSQL from attempting integer casts on string IDs. Both tables checked for deleted file filtering (u1.deleted boolean, u2.deleted_at timestamp). Rebuild endpoint uses async fire-and-forget pattern returning immediately (~200ms) while cache builds in background (typical completion: 300ms-18s depending on month size). November 2025 verified: 16 active files, 135,100 records with complete metrics.

## External Dependencies
- **Database**: PostgreSQL (via @neondatabase/serverless)
- **ORM**: drizzle-orm
- **Web Framework**: express
- **Frontend Library**: react
- **State Management**: @tanstack/react-query
- **Authentication**: passport, @azure/msal-node, @duosecurity/duo_universal
- **UI Components**: @radix-ui/, tailwindcss, lucide-react
- **Routing**: wouter
- **File Uploads**: multer
- **CSV Processing**: csv-parse, fast-csv
- **Cloud Storage**: @aws-sdk/client-s3
- **Scheduling**: node-schedule